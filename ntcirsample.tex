\documentclass{sig-alternate}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{bbm}
%\usepackage[section]{placeins}
% \usepackage{hyperref}
% \hypersetup{
%     colorlinks=true,
%     linkcolor=blue,
%     urlcolor=blue,
%     citecolor=blue,
%     pageanchor=false,
% }
\makeatletter
 \let\@copyrightspace\relax
 \makeatother

\begin{document}

\title{L3S at the NTCIR-12 Temporal Information Access (Temporalia-2) Task}

\numberofauthors{3} %  in this sample file, there are a *total* of EIGHT authors.
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Zeon Trevor Fernando\\
       \affaddr{L3S Research Center, Germany}\\
       \email{fernando@l3s.de}
% 2nd. author
\alignauthor
Jaspreet Singh\\
       \affaddr{L3S Research Center, Germany}\\
       \email{singh@l3s.de}
% 3rd. author
\alignauthor 
Avishek Anand\\
       \affaddr{L3S Research Center, Germany}\\
       \email{anand@l3s.de}
%\and  % use '\and' if you need 'another row' of author names
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\additionalauthors{Additional authors: 
Tom G. Cruise (Disavowed, U.S.A. email: {\texttt{tomg@cruise.com}}) and 
Tom H. Cruise (Disavowed, U.S.A. email: {\texttt{tomh@cruise.com}}).}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.


\maketitle

\begin{abstract}
This paper describes our participation in the NTCIR-12 Temporalia-2 task including Temporal Intent Disambiguation (TID) and Temporally Diversified Retrieval (TDR) subtasks. In the TID subtask, we extract linguistic features from the query, time distance features and multinomial distribution of the query n-grams which are then combined using a rule based voting method to estimate a probability distribution over the temporal intents. In the TDR subtask, we perform temporal ranking based on two approaches, linear combination of textual and temporal relevance method, and learning to rank method. Three classes of features comprising of linguistic, topical and temporal features were used to estimate document relevance in the learning to rank approach.

\end{abstract}

\section*{Team Name}
L3S

\section*{Subtasks}
Temporal Intent Disambiguation Task (English), Temporally Diversified Retrieval Task (English)

\keywords{temporal information retrieval, learning to rank, temporal intent disambiguation, text classification }

\section{Introduction}

Temporal information retrieval is a sub-branch of information retrieval concerned with improving retrieval effectiveness by leveraging temporal information found in documents and queries~\cite{tir,twaw,tir15}. Previous studies have shown that nearly 1.5\% of all queries issued explicitly mention time expressions~\cite{ecir08}, while about 7\% of web queries have an implicit temporal intent~\cite{sigir09}. In the \textsf{TID (Temporal Intent Disambiguation) subtask}, the objective is to estimate a probability distribution of the query intent across four temporal intent classes: \textit{past}, \textit{recent}, \textit{future} or \textit{atemporal}. For example, the query ``history of rap'' implies a high probability should be assigned to the \textit{past} intent. The temporal intent disambiguation of queries is then useful when searching in longitudinal document collections for selecting the appropriate temporal retrieval model~\cite{ecir10,ecdl10,cikm03}. In the \textsf{TDR (Temporally Diversified Retrieval) subtask}, teams must devise retrieval models to produce a ranked list of documents that are diverse across all of the above defined temporal intents for a given query. For a more detailed overview of the subtasks of NTCIR-12 Temporalia-2 task please refer to~\cite{mioverview}.

For the TID subtask, we identify a set of query-specific features such as verb tense of the query, distance between the temporal expressions identified in the query topic and the query hitting time, and the frequently occurring n-grams for each temporal intent using a multinomial distribution. %the multinomial distribution of n-grams of the query.
 We combine them using intent specific rules that if satisfied contribute a vote to the respective intent class. After applying all the rules, votes are aggregated and normalized to determine the distribution across the temporal intents.

In the TDR subtask, our approach is to first classify query subtopics to the correct temporal intent class with high accuracy using a joint classifier (Section \ref{subtopic_classify}). Using the classified subtopics in conjunction with the query topic, we produce a ranked list for each temporal intent class using retrieval models trained by a \textit{listwise} learning-to-rank approach. One of the key features we use is the temporal relevance of a document. Temporal relevance is a score that estimates the expected temporal distance of the document from the query hitting time using the distribution of temporal expressions in the content (refer Section \ref{temp_score}). Finally to produce a diversified list from the top-$k$ results for each class, we use a greedy approach that maximizes the earth mover's distance between the distribution of temporal references in the result set.

%% needs to be fixed. add the right section labels. this is just a sample of how it should be written
\textbf{Outline.} The rest of the paper is organized as follows. In Section \ref{TID} we describe our approach for the TID subtask along with the discussion of the results detailed in Section \ref{sec:tid results}.
% and in \ref{sec:tid rules} the intent specific rules for the TID subtask are explained. Then we discuss the results for the TID subtask in section \ref{sec:tid results}. 
In Section \ref{TDR} we first describe our subtopic classification approach in \ref{subtopic_classify} and then explain how temporal relevance of a document is computed in \ref{temp_score}. We then highlight the features used for the learning-to-rank models in Section \ref{ltor_f}. Our diversification approach using earth mover's distance is the subject of Section \ref{diversification}. In Section \ref{setup} we discuss the experimental setup including the training procedure for the learning-to-rank models. Finally we discuss our performance in the TDR subtask in Section \ref{results} and highlight key takeaways in Section \ref{Conclusion}.





\section{Temporal Intent Disambiguation}\label{TID}
In the TID subtask, given a query (q) and query submission date ($t_{q}$) we have to estimate a distribution across four temporal intent classes (\textit{atemporal, past, recent, future}). Thus to estimate this distribution, we use a rule based voting method that comprises of intent specific rules designed from the dry run queries. The rules use various query-specific features such as verb tense, distance between the date in the query and query submission time and the multinomial distribution of the n-grams extracted from the queries. In this section, we first describe how the features are extracted from the queries and then the approach of how the rule based voting method is used to build the required distribution across the temporal classes.
\subsection{Features extracted for TID}\label{sec:tid features}
\begin{itemize}
\item\textbf{Time Distance Features}: A temporal mention in a query is a good feature to help disambiguate the temporal intent of a query. For example, ``French Open 2012'' becomes a strong indicator of \textit{past} intent given that $t_{q}$ is ``May 1, 2013 GMT+0''. We use the SUTime Library~\cite{sutime} available part of the Stanford CoreNLP pipeline to recognize and normalize temporal expressions in the queries.
%A particular date ``December 2015'' is normalized to ``2015-12'', a less explicit date such as ``Winter 2013'' is represented as a season ``2013-WI''.
%It can also identify relative time expressions, such as, ``two weeks in the future'' which is normalized to ``offset P2W''. Simple temporal words such as ``now'' and ``the future'' are indicated as present or future reference respectively, but this leads to false indicators like in the case of ``the power of now'' or ``future shop''.
The distance of the normalized time expressions from the query hitting time is measured to provide an estimate of the intent.

In the dry run queries only 16 out of 100 contained time expressions that can be used to estimate the time distance, this reflects how rare it is to find explicit or implicit temporal expressions in a search query. Thus in order to obtain more candidate temporal expressions for the formal and dry run queries, we used the freely accessible GTE\footnote{http://www.ccc.ipt.pt/~ricardo/software.html} web service detailed in \cite{gte}. Given a query, the GTE web service returns a set of candidate years extracted from the \textit{top 50 web snippets} returned by the Bing Search API.  
\item\textbf{Linguistic Features}:
Verb tense is a strong temporal indicator for search queries~\cite{tuta}. For example, the verb ``was'' in ``When \textit{was} the first Olympics held'' is a strong indicator of \textit{past} intent. We use the Stanford Part-of-Speech Tagger library~\cite{postagger} that recognizes verbs along with the tenses in a sentence using the Penn Treebank tag set. Verb tenses included in the Penn Treebank set include past tense, past participle, present tense, present participle and base verb form. Since the Penn Treebank set doesn't include any tags for future tense, we consider a \textit{base} verb that is preceded by a \textit{modal} verb such as \textit{will/shall} to be an indicator of \textit{future} tense. A query can include multiple verbs with different tenses. Thus we do a syntactic parsing of the query using the Stanford Parser Library~\cite{parser} to determine the main predicate, by selecting the uppermost verb in the parse tree. 
\item\textbf{NGram Features}:
As a set of baseline features, we extract the uni-gram and bi-gram terms of the queries from the training data, which is 73 dry run queries. We model the per class multinomial distribution of the \textit{n-grams} by using the n-grams overall frequency (\textit{T}) and the per class n-gram count (\textit{C}). The per class n-gram count is computed by counting the n-grams per class (like past) generated from those queries which have a non-zero past probability in the training data.
\begin{equation}\label{eq:1}
p(ng, class) = \frac{C}{T}
\end{equation}
Each formal run query (\textit{q}) is represented as a set of all possible permutations of the uni-gram and bi-gram terms that are extracted from the query. Then the probability distribution of a query across the temporal classes is calculated using the following equation.
\begin{equation}\label{eq:2}
p(q,class) =\underset{i\in q}{\arg\max}\prod_{ng \in i}p(ng, class)
\end{equation}
\end{itemize}
\subsection{Rule Based Voting Method}\label{sec:tid rules}
We designed rules based on the dry run queries, which if satisfied contributes a vote to a particular temporal class. Once all the rules are applied, the votes are accumulated for each temporal class and a probability distribution across the temporal classes are built using these votes. We apply the rules in the same order as defined below.
\begin{itemize}
\item \textsf{NGram features} provides the probability of a query to a particular temporal class. We learn a decision tree model based on regression for the dry run queries using the NGram features. This learned model is then used to predict the probabilites for the formal run queries. The temporal class with the maximum probability output from the learned model gets a vote.
\item From the \textsf{verb tense features}, if the tense of a query is either past, present or future, a vote is assigned to that respective temporal class~\cite{hitsz}. If no tense information can be extracted from the query, a vote is assigned to the atemporal class.
\item If the \textsf{temporal mention} extracted from the query contains the words (\textit{``past\_ref'', ``present\_ref'' or ``future\_ref''}), a vote gets assigned to that temporal class. If the temporal mention is a date and if its earlier or after the query hitting time, a vote gets assigned to past and future class respectively.
\item If after applying the above rules, the votes across all the temporal classes are low and their standard deviation is low. We then take the mean of the candidate years extracted for the query from the \textsf{GTE service}. If the difference between the mean and the query hitting time is about 1 year in the past, we give a vote to recent. If the difference is more than 2 years, a vote is assigned to the past. If the mean date is after the query hitting date, a vote is given to future. If there are no candidate years, atemporal gets a vote.
\end{itemize}
\subsection{Experiments}
\subsubsection{Temporal Intent Disambiguation Runs}
We submitted 2 runs for the TID subtask.
\begin{itemize}
\item \textsf{L3S-TID-E-1}: This run uses the \textsf{rule based voting method} to generate the probability distribution across the temporal classes for a given query.
\item \textsf{L3S-TID-E-2}: This run uses the probability distribution across temporal classes for a query generated only from the \textsf{NGram features}.
\end{itemize}
\subsubsection{Results and Discussion}\label{sec:tid results}
\begin{table}[htbp]
\centering
\begin{tabular}{|c|c|c|}
\hline
Run & Average Absolute Loss & Cosine Similarity \\
\hline
\hline
L3S-TID-E-1 & \textbf{0.2031} & \textbf{0.7307} \\
\hline
L3S-TID-E-2 & 0.2452 & 0.6673 \\
\hline
\end{tabular}  
\caption{Evaluation Results of TID Formal Runs}
\label{table:1}
\end{table}
The evaluation results in \textsf{table \ref{table:1}}, show that the \textsf{rule based voting} method does better than the baseline which uses only \textsf{NGram features} across both performance measures. This highlights how the \textsf{linguistic} and \textsf{time distance} features helps improve the estimation of the temporal intent of a query. Our method helps determine the correct distribution for queries that contain linguistic information such as \textit{``what was the great awakening''} and \textit{``when did elvis die''}. However, for queries that have an implicit/explicit temporal expression identified, such as, \textit{``uk 2009 balance of payments''} or \textit{``John Galliano Spring 2011 Mens Preview''} the probabilities are spread across all temporal classes with higher probability for the past intent. This happens since we aggregate the votes for time distance only after accumulating the votes for NGram and linguistic features. Consequently, this increases the average absolute loss because these type of queries clearly indicate a particular temporal intent, thus for queries that contain temporal expressions only the time distance rule should be applied. 

Using a rule, based on dictionary words such as \textit{``future'', ``schedule'', ``releases'', ``forecast'', ``plan''} could help estimate queries that have higher probability of future intent. Certain queries indicating future intent were misclassified as past intent because of the ambiguity introduced by the temporal tagger. For example, \textit{``November Calendar printable''} and \textit{``December calendar''} were resolved to ``2012-11'', ``2012-12'' respectively. 

The probability for atemporal queries like \textit{``causes of global warming'', ``light pollution'', ``literature critics''}, were easily estimated as all the features contributed votes to the atemporal intent and also no candidate years were returned by the \textsf{GTE service} indicating atemporal nature. However, for atemporal queries like \textit{``consumer economy'', ``sherlock holmes''}, the distance of the mean of the candidate years returned by \textsf{GTE service} indicates past, which causes a misclassification. Thus we should re-design the rule to also consider the spread of candidate years across time to understand the atemporal intent. Also the candidate years returned by the \textsf{GTE service} is based on search results returned in December 2015, which won't be optimal for queries with issuing time of 2013.

Queries like \textit{``how long does the flu last'', ``the advantages of hosting the olympic games''} are considered to be either past or recent using the \textsf{NGram} and \textsf{linguistic} features, in these cases we need additional rules to help disambiguate the atemporal intent. 

\section{Temporally Diversified Retrieval}\label{TDR}
In the TDR subtask, we are given a topic, description and an indicative search question (subtopic) for each temporal class and we have to retrieve a list of relevant documents for each of these classes. We also have to return a list of documents that are temporally diverse for the same topic. Since the subtopic class information was not supposed to be used, we use a classifier to jointly classify the subtopics to the respective intents based on verb tense and dictionary features detailed in Section \ref{subtopic_classify}. Once we have a classified subtopic, we use it for retrieval which is based on a \textsf{learning-to-rank} approach using features extracted from verb tenses of sentences in documents, topical similarity of the document, textual relevance returned by statistical language model and temporal features based on the distribution of time references in the document with intent specific filters.

Documents provide temporal information in two forms, publication date and temporal expressions in the content. Thus in Section \ref{temp_score}, we describe how to compute the temporal relevance score of a document using the temporal expressions in the content. We then shortly describe in Section \ref{linear_comb}, how this temporal relevance score is combined with the topical relevance score in a parameterized sum. Next, we focus on the features extracted from the document that are used in learning-to-rank approaches to build different ranking models for each temporal intent (Section \ref{ltor_f}). Finally, we describe in Section \ref{diversification} the approach for producing the diversified set of documents across all temporal intents that uses the set of documents considered relevant for each temporal intent as candidates. 
\subsection{Subtopic Classification}\label{subtopic_classify}
For each search topic, the workload also contains subtopics which are indicative search questions for each of the temporal classes. We use a multiclass SVM classifier\footnote{https://www.cs.cornell.edu/people/tj/svm\_light/} to classify subtopics using the features extracted from it. Using this classifier there is a likelihood that subtopics belonging to the same topic will be classified to the same temporal intent. Thus we use the confidence score returned from the SVM classifier to jointly classify the subtopics for a given topic using a greedy approach.

In this approach we first pick the subtopic-intent pair that has the maximum confidence score, and then ignore the confidence scores of this intent for the remaining subtopics. Then we pick the next highest confidence score pair from the remaining set and proceed as above to determine all unique subtopic-intent pairs.

The features that are used for the multiclass classifier are listed below.
\begin{itemize}
\item We extract the tense of the subtopic similar to the approach described in section \ref{sec:tid features} which is used for the TID subtask. 
%The only difference is that if the tense of the root verb is different from the tense of the following verb we choose the tense of the last verb in the subtopic to be its tense.
\item We identified certain words from the dry run queries which were frequently occurring for certain temporal intents and built a word dictionary. We also included synonyms of the above identified words and the class names as well.
% \begin{itemize}
% \item Future: future, forecast, will, would, should, shall, next, expected, soon, projected, possibility, scheduled
% \item Past: past, history, were, origin, did, been, previous, earlier, former, historical
% \item Recent: recent, present, current, latest, recently, trendy, now, today
% \end{itemize}
\item We compute the average expected distance (section \ref{temp_score}) for the subtopic from the top 20 pseudo relevant documents that were retrieved. 
\end{itemize}
\subsection{Temporal Relevance Score of Document} \label{temp_score}
The temporal relevance estimates the expected distance of a document in time, that is, the focus time using the temporal distribution of time expressions in the content. Thus in this section, we describe how we compute this temporal relevance using the annotated normalized time expressions $\tau(d)$. We map each time expression \textit{$t_{e}$} in the document \textit{d} to a time interval [$b,e$) at day granularity (e.g., May 2014 is mapped to [01/05/2014, 31/05/2014]). Then we determine the temporal distribution of time references in a document at monthly granularity, that is, each document $d$ is represented by a set $\tau_{m}(d)$ of monthly time intervals [$t_{mb},t_{me}$). The weight of each monthly time interval is calculated as follows
\begin{equation}\label{eq:3}
\resizebox{0.98\hsize}{!}{$
              w([t_{mb},t_{me})) =\sum\limits_{t_{e} \in \tau(d)} \begin{cases}               
               \frac{cnt(t_{e})}{|\tau(d)|}, & t_{e} \in [t_{mb},t_{me})\\\\
               \frac{cnt(t_{e})}{|\tau(d)|} \cdot g, &  [t_{mb},t_{me}) \in t_{e} \in \{t_{g}\}
            \end{cases}
            $}
\end{equation}
where $cnt(t_{e})$ is the count of the number of occurrences of $t_{e}$ in the document. The constant $t_{g}$ is used to describe if a time interval is at a yearly granularity and the constant $g$ is the weight contributed by this temporal expression to the time interval [$t_{mb},t_{me}$)(i.e.$\frac{1}{12}$).

The intent specific filter ($\mathcal I$) for recency ($\mathcal R$), past ($\mathcal P$) or future ($\mathcal R$) are chosen based on the subtopic classification. It is modeled as an exponential distribution using $dist$ as the distance between query issue time and time expression measured in months ($t_{q} - t_{e}$):
\begin{equation}\label{eq:4}
f(t_{e}) = \begin{cases}
                     \lambda e^{-\lambda |dist|} \cdot \mathbbm{1}(dist \geq 0), & if\ \mathcal I = \mathcal R\\
                     \lambda e^{\lambda |dist|} \cdot \mathbbm{1}(dist > 0), & if\ \mathcal I = \mathcal P\\
                     \lambda e^{\lambda |dist|} \cdot \mathbbm{1}(dist < 0), & if\ \mathcal I = \mathcal F
              \end{cases}
\end{equation}
where $\mathbbm{1}(dist \geq 0)$ is an indicator function whose value assumes 1 iff $dist \geq 0$. $\lambda$ is a tunable parameter, which is set to 0.03 in our experiments. The intuition behind this approach is that temporal expressions close to $t_{q}$ have a higher probability than older temporal expressions for the recency filter. While for the past and future filter, temporal expressions further away from $t_{q}$ have a higher probabilty. 

We then use the chosen intent specific filter to transform the temporal distribution of time references in a document.
\begin{equation}\label{eq:5}
h(t_{e}) = \begin{cases}
              w(t_{e}) \cdot f(t_{e}) \cdot |dist|, & if\ \mathcal I\ = \mathcal P\ or\ \mathcal F\\
              w(t_{e}) \cdot f(t_{e}) \cdot \frac{1}{|dist|}, & if\ \mathcal I\ = \mathcal R
           \end{cases}
\end{equation}
So the expected distance of the document with respect to the query hitting time, i.e., temporal relevance score is computed as follows

\begin{equation}\label{eq:6}
       E(d) = \tfrac{1}{|\tau_{m}(d)|}\sum\limits_{t_{e} \in \tau_{m}(d)}h(t_{e})
\end{equation}
\subsection{Parameterized Sum Method}\label{linear_comb}
 For all our experiments, we determine a set \textit{R} of pseudo-relevant documents (|\textit{R}|=1000) by employing a unigram language model with Dirichlet smoothing ($\mu$ = 2000)~\cite{lm}. We then re-rank the documents using scores obtained from the linear combination of the temporal relevance and topical relevance score, defined as follows
 \begin{equation}\label{eq:7}
 R_{f} = \lambda E(d) + (1 - \lambda) R_{c}, \ \ 0 \leq \lambda \leq 1
 \end{equation}
where $\lambda$ is a tunable parameter and $R_{c}$ is the relevance score of the language model.
\subsection{Learning-To-Rank Features}\label{ltor_f}
%In learning-to-rank approaches a ranking model is built by training a set of query-document pairs using a learning algorithm. The learned model is a weighted coefficient $w$ of a feature vector $x$, which can then be used to rank unseen query-document pairs. 
In our approach we use a \textit{listwise} learning-to-rank algorithm optimized for the evaluation measure $nDCG@20$. For a detailed description of the different approaches, refer to~\cite{ltor}. Feature selection is critical for learning-to-rank approaches, so in this section we describe the various features that we extract from the query-document pairs.
\begin{itemize}
\item\textbf{Verb Tense Features}: We take the noun terms of the search query into consideration and split the document into two sentence types: $\mathcal S_{noun}$ those sentences that contain atleast a noun search term and $\mathcal S_{non-noun}$ those that don't contain any noun search term. We determine if a sentence talks about the past, present or future by using the same approach as the linguistic feature for TID subtask (Section \ref{sec:tid features}). We thus have \textsf{6 verb tense features}: 
\begin{itemize}
\item the ratio of past, present and future tense w.r.t $\mathcal S_{noun}$,
\item the ratio of past, present and future tense w.r.t $\mathcal S_{non-noun}$. 
\end{itemize}
These features help determine the language of the document, how much of the text talks about the past, present or future which in turn helps match it to a particular temporal intent. 
\item\textbf{Topical Features}: These include \textsf{4 similarity based features} using the jaccard similarity on a word level between:
   \begin{itemize}
       \item search topic and document title,
       \item search topic and document content,
       \item search subtopic and document title, 
       \item search subtopic and document content. 
   \end{itemize}
These features help determine the topical similarity between the document, search topic and subtopic. We also use the \textsf{document relevance score} between the search query and the document as a feature. The relevance score obtained from the unigram language model with Dirichlet smoothing is directly used. 
\item\textbf{Temporal Features}: These include $two$ features based on the temporal expressions of the document.
       \begin{itemize}
       \item \textsf{temporal relevance score} computed for a document as described in Section \ref{temp_score}.
       \item \textsf{temporal density feature} which is the ratio of the number of temporal expressions to the length of the document. This helps differentiate between atemporal and temporal documents. 
       \end{itemize}
\end{itemize}
\subsection{Earth Mover's Distance for Diversification}\label{diversification}
The earth mover's distance ($\mathcal E$) is a measure of distance between two probability distributions, it is the minimum cost required to transform one probability distribution to another. In our case, we measure the $\mathcal E$ between the temporal distribution of time references from one document to another.  We use $\mathcal E$ for diversification, so that we get a set of documents that have diverse temporal distributions which would in turn give a temporally diversified set. We consider sets $R_{i}$ containing candidate documents from the top 100 documents retrieved for the specific intent ($i \in$ \{$atemporal$, $recency$, $past$, $future$\}) using the above ranking approaches. We represent the diversified set of results as $R_{D}$, to which we add documents from sets $R_{i}$ that have maximum $\mathcal E$ from the documents already present in $R_{D}$. During the initial step, we add a $rank\ 1$ document from one of the sets $R_{i}$ at random and initialize $\mathcal E_{0}$ with a zero value. Then we compute the $\mathcal E$ between two temporal distributions $A$ and $B$ as follows
\begin{equation}\label{eq:8}
	\begin{aligned}
		\mathcal E_{i+1} &= (w_{A}(te_{i}) + \mathcal E_{i}) - w_{B}(te_{i}) \\
		\mathcal E_{Total} &= \sum\limits_{i=1}^{|\tau_{m}(A) \cup \tau_{m}(B)|}|\mathcal E_{i}|
	\end{aligned}
\end{equation}
We give preference to top ranked documents to be added to $R_{D}$ by discounting the $\mathcal E_{Total}$ using the rank of the document, so the final score that we consider is
\begin{equation}\label{eq:9}
       \mathcal E_{f} = \frac{1}{rank} * \mathcal E_{Total}
\end{equation}
\subsection{Experimental Setup}
\label{setup}
We used Lucene\footnote{https://lucene.apache.org/core/} to build the index for the ``LivingKnowledge news and blogs annotated subcollection'' corpus~\cite{mioverview}. The unigram language model with Dirichlet smoothing implementation of Lucene was used to retrieve the top 1000 pseudo-relevant documents. The query is constructed from the title of the topic and the subtopic, and then searched against the title and content fields of the documents. The features described in section \ref{ltor_f} are extracted from the tagged version of a pseudo-relevant document.

\textbf{Training data}. The 10 dry run topics and the 50 formal run topics of Temporalia-1 along with their $qrels$\footnote{http://research.nii.ac.jp/ntcir/permission/ntcir-11/perm-en-Temporalia.html} are used to generate the training data for learning the ranking models. Each row in the training data is a query-document pair: the first column is the relevance judgement, the second is query id (qid) used to restrict the generation of constraints, the subsequent columns are feature/value pairs ordered by increasing feature number. We created separate training datasets for each temporal class, the data is prepared as follows: the query containing topic and subtopic is used to retrieve the top-1000 pseudo relevant documents using a language model (LM). The relevance judgements in the $qrels$ are of the order 2 (\textit{really relevant}), 1 ($relevant$) and 0 ($irrelevant$). From the pseudo relevant documents, we selected relevant and irrelevant documents in different ratios (1:1, 1:2, 1:3) in order to find the right balance of training examples. Finally we found that the ratio 1:2 (relevant: irrelevant) for preparing the training data performs best. Each temporal class-specific training data is then used to learn a ranking model so as to predict document ranking for a formal-run subtopic of the same temporal class. Besides, we also experimented by combining all the class-specific training data into one large training set, but the performance was lower than the class-specific training data.

\textbf{Ranking Models}. The RankLib\footnote{https://sourceforge.net/p/lemur/wiki/RankLib/} library is used to compare different \textit{listwise} learning to rank approaches, such as, AdaRank~\cite{adarank}, RankBoost~\cite{rankboost} and LambdaMART~\cite{lambdamart}. We used the default learning algorithm specific parameters and optimize for the measure $nDCG@20$. For the final runs, we used the AdaRank learning algorithm.
\subsubsection{Temporal Diversified Retrieval Runs}
We submitted 3 runs for the TDR subtask. For all the runs the diversification of the results across all temporal intents is carried out using the earth mover's distance measure.
\begin{itemize}
\item \textsf{L3S-TDR-E-1}: Manual run with manually crafted queries using the topic and subtopic. The training and test data (formal runs) is generated from the pseudo-relevant documents retrieved using LM. The ranking model is learned based on the class-specific datasets.
\item \textsf{L3S-TDR-E-2}: Automatic run in which the subtopics are classified using the joint classifier described in Section \ref{subtopic_classify}. The pseudo relevant documents are retrieved using LM and then re-ranked using the parameterized sum method (Section \ref{linear_comb}). The parameter $\lambda$ is set to 0.3, giving more weightage to the textual relevance score.
\item \textsf{L3S-TDR-E-3}: Automatic run in which the subtopics are classified using the joint classifier described in Section \ref{subtopic_classify}. The training and test data (formal runs) is generated from the pseudo-relevant documents retrieved using LM. The ranking model is learnt based on the class-specific datasets.
\item \textsf{LM}: In this run documents are retrieved using LM.
\end{itemize}
\subsubsection{Results and Discussion}
\label{results}
\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|}
\hline
Run & D\#-NDCG@20 & I-rec@20 \\
\hline
\hline
L3S-TDR-E-1 & 0.8262 & 0.9850 \\
\hline
L3S-TDR-E-2 & 0.6852 & \textbf{0.9900} \\
\hline
L3S-TDR-E-3 & \textbf{0.8423} & 0.9850 \\
\hline
\end{tabular}  
\caption{Diversified Results of TDR Formal Runs}
\label{table:3}
\end{table}
\begin{table*}[htb]
%\centering
\begin{tabular}{ |c|c|c|c|c|c|c|c|c|c|c| }
 \hline
  \multirow{2}{*}{Run} & \multicolumn{5}{|c|}{NDCG@20} & \multicolumn{5}{|c|}{P@20}\\
  	\hhline{~----------}
    & Atemporal & Future & Past & Recency & All & Atemporal & Future & Past & Recency & All \\
    \hline
 \hline
 %\multirow{2}{Run} &
 %\multicolumn{5}{|c|}{NDCG@20} \\
 %& Atemporal & Future & Past & Recency & All \\
 %\hline\hline
 L3S-TDR-E-1 & 0.7264 & 0.6511 & 0.7005 & \textbf{0.7151} & 0.6983 & \textbf{0.7960} & 0.7360 & 0.7710 & \textbf{0.7970} & \textbf{0.7750}\\ 
 \hline
 L3S-TDR-E-2 & 0.6109 & 0.6932 & 0.7127 & 0.6758 & 0.6731 & 0.7330 & 0.7790 & \textbf{0.8000} & 0.7760 & 0.7720\\ 
 \hline
 L3S-TDR-E-3 & \textbf{0.7299} & 0.6508 & 0.6998 & 0.7116 & 0.6980 & 0.7960 & 0.7360 & 0.7700 & 0.7930 & 0.7737\\ 
 \hline
 LM & 0.7052 & \textbf{0.7151} & \textbf{0.7297} & 0.6865 & \textbf{0.7076} & 0.7690 & \textbf{0.7850} & 0.7940 & 0.7580 & 0.7416\\ 
 \hline
\end{tabular}
\caption{Per-Class results for all TDR Runs. For every temporal class, the highest value is indicated in bold.}
\label{table:2}
\end{table*}

The evaluation results in table \ref{table:2} shows that (1) there is no significant difference in overall performance between the manual run (\textsf{L3S-TDR-E-1}) and automatic run with subtopic classification (\textsf{L3S-TDR-E-3}) which indicates that the joint classification approach for subtopic classification performs well. (2) The \textit{nDCG@20} performance for the \textit{atemporal} class is poor for parameterized sum method when compared to the learning-to-rank approach by about 19\%. This shows that the topical features based on similarity measures along with the temporal density feature helps significantly retrieve more relevant atemporal documents. (3) The \textit{nDCG@20} performance for the \textit{future} intent is higher using parameterized sum method than using learning-to-rank approach by about 7\%, this indicates that using topical features based on similarity measures and verb-tense features have a deterimental effect than only using the temporal relevance score when retrieving documents for future intent. (4) All our models perform better than the baseline in terms of rank insensitive metric \textit{P@20}. However, in terms of \textit{nDCG@20} the performance for \textit{future} and \textit{past} class of the baseline outperforms all our models, indicating the importance of textual relevance in retrieving such documents. Our learning-to-rank models outperform the baseline in \textit{nDCG@20} for the \textit{atemporal} and \textit{recency} class, highlighting the significance of temporal features such as temporal relevance and temporal density for these classes. 

The overall \textit{ERR} measure is high, indicating that the 1st and 2nd ranked document is relevant for most topics across all runs. The topic ``The right to be forgotten'' performs poorly for \textit{ERR} and \textit{nDCG@20} measures across all subtopics since we apply stopword removal to the topics before forming a query (e.g.``right forgotten''), which when used doesn't retrieve the most relevant documents for this topic highly enough.

The performance of the diversified results is measured using \textit{D\#-nDCG@20} that combines intent recall (\textit{I-rec@20}), and \textit{D-nDCG@20} which is a form of nDCG measure where the gain is replaced with a global gain value that takes into consideration the per intent graded relevance for a document. Our diversification approach performs well across learning to rank runs (table \ref{table:3}), since the intent recall is high across all topics as we consider the top 100 relevant documents returned for each temporal class and since our \textit{nDCG} values are relatively high for the individual temporal classes as well. The earth mover's distance works well in our case since we diversify the candidate documents by selecting those documents that are highly ranked in the individual lists and those that are most diverse in the temporal distribution of the time references. Our method could be improved, if instead of a random selection for selecting the first document, we use a method to choose the most confident document from across the temporal classes. Also, considering only the top 20 relevant documents from each intent could improve the \textit{D\#-nDCG@20} measure.
%Regarding the d# measure for diversity, it takes a linear combination of Intent recall and D-ndcg. D-ndcg replaces the gain measure of NDCG with a global gain that takes into consideration the per intent graded relevance for a document.
%\begin{itemize}
%\item Difference between manual and automatic run with subtopic classification
%\item atemporal using parameterized sum is poor compared to learning-to-rank by 0.15 or 23\%
%\item future using parameterized sum is better by 0.06 or 11\%
%\item Effect of temporal features 
%\item ERR is high indicating 1st and 2nd document is always relevant
%\item diversification results 
%\end{itemize}
\section{Conclusions}\label{Conclusion}
In this paper we discussed our approaches for solving the TID and TDR subtask part of the Temporalia-2 task. For the TID subtask, we used a rule-based voting method that is comprised of intent specific rules which use query-specific features such as verb tense of the query, temporal expression identified from the query and the multinomial distribution of n-grams in the query. The evaluation results show that incorporating the linguistic and temporal features helps improve the estimation of the temporal intent of a query. Adding a rule, based on dictionary words related to future could help improve the estimation of queries that have a higher probability for future intent. For atemporal queries that have candidate years returned from the GTE service, rules which determine the spread of years across time is needed for disambiguation.

In the TDR subtask, we jointly classified all the subtopics together to get unique subtopic-intent pairs. We then built separate learning to rank models for each temporal intent using features extracted from the document. The temporal relevance score helped improve the performance measures for future and past intents. The overall performance could be improved by mining other indicative search questions for an intent, as well as using the named entity tags to improve content relevance and to determine important time expressions.

The temporal diversification of results using earth mover's distance was an effective approach exploiting the temporal distribution of time references in the documents. However, this could be improved by experimenting with smaller candidate sets of relevant documents from each individual intent.
\bibliographystyle{abbrv}

\bibliography{ntcirsample}

\end{document}

